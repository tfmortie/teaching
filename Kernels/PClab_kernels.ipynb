{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear basis functions and kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PC-lab, we will have a look at two particular strategies in order to be able to analyze non-linear datasets, which are called _linear basis functions expansion_ and _kernels_. \n",
    "\n",
    "We will first have a look at linear basis functions. Next we will turn a logistic regression model to a kernel logistic regression model ourselves. Finally we will study kernel ridge regression on a real-world dataset, where we will predict the compressive strength of concrete based on its composition and age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to solve a binary classification problem, classifiers usually try to separate the positive from the negative class. Often, linear models are not capable of making this separation. An option to deal with this problem is to find an (extended) basis in which the classes are (almost) linearly separable. \n",
    "\n",
    "As an example, consider the binary classification problem in the figure below. The data depicted in this figure originates from a(n) (unknown) joint distribution, for which the Bayes-optimal decision boundary (in case of the 0/1 loss) is a circle. It is clear that the classes are not linearly separable in the original feature space (which is $\\mathbb{R}^2$). An option is to extend this space using a set of non-linear functions $\\phi_1 (\n",
    "x_1,x_2), ... , \\phi_m (x_1,x_2)$ (note that the identity map can also be chosen for $\\phi_i$). Using these transformations, the representation of an instance $\\mathbf{x_i} = (x_{i,1},x_{i,2})$ becomes $\\mathbf{x_i} = (x_{i,1},x_{i,2}) \\rightarrow \\phi_1(x_{i,1},x_{i,2}),..., \\phi_M(x_{i,1},x_{i,2})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1 (warm-up): \n",
    "<br>\n",
    "a) Generate a dataset according to the code that is given. Have a look at the documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html). \n",
    "<br>\n",
    "b) Which features would you construct in order to separate these two classes? \n",
    "<br>\n",
    "c) Use logistic regression in order to make sure your features are working (make sure you evaluate the performance on a separate test set). In other words, fit a logistic regression model on both the original features and your new features. What's the difference in accuracy between the two? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##1a: \n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_train, labels_train = make_circles(n_samples=400, noise=0.1, factor=0.6, random_state=23)\n",
    "circles_test, labels_test = make_circles(n_samples=400, noise=0.1, factor=0.6, random_state=85)\n",
    "blues = labels_train == 0\n",
    "reds = labels_train == 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(circles_train[reds,0],circles_train[reds,1], c='r');\n",
    "ax.scatter(circles_train[blues,0],circles_train[blues,1], c='b');\n",
    "ax.set_xlabel(r'$x_1$', size=18)\n",
    "ax.set_ylabel(r'$x_2$', size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b: \n",
    "#Complete the code: \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1c: \n",
    "\n",
    "#Fit logistic regression: \n",
    "\n",
    "\n",
    "#Code to display grid: \n",
    "x1, x2 = np.mgrid[-2:2:.01, -2:2:.01]\n",
    "grid = np.c_[x1.ravel(), x2.ravel()]\n",
    "#Z = ... #predictions of grid\n",
    "Z = Z.reshape(x1.shape)\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(x1, x2, Z, cmap=plt.cm.Paired)\n",
    "ax.scatter(circles_train[reds,0],circles_train[reds,1], c='red');\n",
    "ax.scatter(circles_train[blues,0],circles_train[blues,1], c='blue');\n",
    "ax.set_xlabel(r'$X_1$', size=18)\n",
    "ax.set_ylabel(r'$X_2$', size=18)\n",
    "plt.show()\n",
    "print('Accuracy original features: ' + str(acc_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods exist to perform classification or regression tasks in an extended space. A general approach of doing so, is by using so-called _kernels_. By using a computational shortcut, which is called the _kernel trick_, an explicit calculation of this extended space is not needed, and using a dot-product between two input vectors $\\mathbf{x_i}$ and $\\mathbf{x_j}$ suffices. The most simple kernel is the _linear kernel_, for which the kernel function looks as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = \\mathbf{x_i}^T\\mathbf{x_j}. \n",
    "\\end{equation}\n",
    "\n",
    "Consider the following polynomical kernel function given by: \n",
    "\\begin{equation}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = (\\mathbf{x_i}^T\\mathbf{x_j})^2. \n",
    "\\end{equation}\n",
    "\n",
    "Let again be $\\mathbf{x_i} = (x_{i,1},x_{i,2})$. Then we can give a simple illustration of the kernel trick by writing: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) &= (\\mathbf{x_i}^T\\mathbf{x_j})^2 = (x_{i,1}x_{j,1} + x_{i,2}x_{j,2})^2 \\\\\n",
    "        &= x_{i,1}^2x_{j,1}^2 + x_{i,2}^2x_{j,2}^2 + 2x_{i,1}x_{j,1}x_{i,2}x_{j,2} \\\\\n",
    "        &= (x_{i,1}^2, x_{i,2}^2, \\sqrt{2} x_{i,1}x_{i,2})(x_{j,1}^2, x_{j,2}^2, \\sqrt{2} x_{j,1}x_{j,2}) \\\\\n",
    "        &= \\phi(\\mathbf{x_i})^T\\phi(\\mathbf{x_j}). \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the squared term of the dot product of vectors $\\mathbf{x_i}$ and $\\mathbf{x_j}$ is equivalent to the the product of the explicit feature mapping $\\mathbf{\\phi(x_i)}$ and $\\mathbf{\\phi(x_j)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular kernel is the _radial basis functions (rbf)_ kernel. The feature space implied by this kernel is infinite-dimensional: \n",
    "\\begin{align}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = \\left( \\exp{- \\frac{||\\mathbf{x_i}-\\mathbf{x_j}||^2}{2\\sigma^{2}}} \\right). \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _kernel methods_ in machine learning make use of the kernel (Gramm) matrix $K$, which is defined as: \n",
    "\\begin{equation}\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "    k(\\mathbf{x_1},\\mathbf{x_1}) & ... & k(\\mathbf{x_1},\\mathbf{x_n}) \\\\\n",
    "    \\vdots & & \\vdots \\\\\n",
    "    k(\\mathbf{x_n},\\mathbf{x_1}) & ... & k(\\mathbf{x_n},\\mathbf{x_n}) \\\\\n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "where we have $n$ training instances at our disposal. Once the model is fit, it can be\n",
    "used to predict the labels of new instances uses a kernel matrix of the following form, for which test instances are denoted with a $*$: \n",
    "\\begin{equation}\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "    k(\\mathbf{x_1}^*,\\mathbf{x_1}) & ... & k(\\mathbf{x_1}^*,\\mathbf{x_n}) \\\\\n",
    "    \\vdots & & \\vdots \\\\\n",
    "    k(\\mathbf{x_L}^*,\\mathbf{x_1}) & ... & k(\\mathbf{x_L}^*,\\mathbf{x_n}) \\\\\n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "where we now have $L$ test instances. Using a kernel gives rise to a whole scala of machine learning methods: \n",
    "- Support Vector Machines (previous lecture)\n",
    "- Kernel Principal Component Analysis\n",
    "- Kernel Logistic Regression (this lecture)\n",
    "- Kernel Ridge Regression (this lecture)\n",
    "- ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 2: \n",
    "<br>\n",
    "a) Implement the rbf-kernel and calculate the kernel matrix for the previous dataset. In other words, expand the dataset from the first exercise to the new 'kernel' space. (Tip: You can do this yourself, or use your beloved machine learning library). \n",
    "<br>\n",
    "b) Fit a logistic regression model to the expanded features, using the kernel matrix. In this way, we are building our own kernel logistic regression model. What is the performance on the test set?\n",
    "<br>\n",
    "c) Which other kernels might work for this dataset? \n",
    " </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2a): \n",
    "\n",
    "#Calculate the kernel matrix using the rbf-kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2b): \n",
    "\n",
    "#Fit a logistic regression model using the kernel matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, lots of methods make use of kernels. Another example is ridge regression, which can be extended to kernel ridge regression (KRR) in quite a standard way. By making use of the kernel trick, the method will fit a linear model (using an $l_2$-penalty) in the expanded kernel space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now have a look at the 'concreteComprStrength.txt' dataset. This dataset can be used to predict the compressive strength of concrete based on its composition. Have a look at the file 'concreteComprStrength.info' for more information concerning the variables. The target variable to predict is the 'Concrete compressive strength'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3: \n",
    "<br>\n",
    "a) Look at the implementation of ridge regression and kernel ridge regression. Implement both of them to analyze the `concrete` dataset. For KRR, try both the polynomial and rbf-kernel. Which one works better? Evaluate your performance on a 30% held-out test set in terms of the mean squared error (MSE). \n",
    "<br>\n",
    "b) KRR can be tweaked in various ways. How many hyperparameters do you have? Choose the polynomial kernel and start doing this by hand to get a feel of the hyperparameters. What's the influence of each hyperparameter? Can you visualize this? \n",
    "<br> \n",
    " </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3a: \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/Kernels/concreteComprStrength.txt\n",
    "df = pd.read_table('concreteComprStrength.txt', delim_whitespace=True, header=0, index_col=None)\n",
    "df = df.sample(frac=1)\n",
    "features = ['cement', 'blastFurnaceSlag', 'flyAsh', 'water', 'superelastizer', 'coarseAggregate', 'fineAggregate', 'age']\n",
    "target = ['compressiveStrength']\n",
    "\n",
    "#Complete task here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3b: \n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
