{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a033a991-9066-425e-b842-84cb72288e2a",
   "metadata": {},
   "source": [
    "# PC lab: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed54e2d-c0ef-4b5e-8fe3-7551baa4f4c8",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701d14b-e08f-48be-b011-5063525017c7",
   "metadata": {},
   "source": [
    "In a binary classification setting, we are interested in assigning an observation ùê± to one of two possible classes, denoted by ùë¶. For example, maybe we would like to tell if a patient has a particular disease (y = 1) or not (y = 0), given certain symptoms ùê±, in other words, we want to model $P(y|\\mathbf{x})$, also called the *class posterior* or the *class-membership probability*. Two main approaches exist to model this probability: either directly using *discriminative models* or using *generative models*, which first model the joint probability $P(\\mathbf{x},y)$ and obtain the class posterior using Bayes' rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666eb86a-66f7-46ec-9305-e7b92cc1c0a4",
   "metadata": {},
   "source": [
    "### 1.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8826e63-fbe7-4bba-ac02-d71ec7f1c7d9",
   "metadata": {},
   "source": [
    "**Logistic regression** (LR) is the most fundamental example of a **discriminative** model. Just like linear regression, LR is a linear model. However, LR does not model the mean of a continuous outcome, but the logarithm of the [odds](https://en.wikipedia.org/wiki/Odds) of the probability $P(X)$:\n",
    "\n",
    "\\begin{equation}\n",
    "log \\frac{P(y|\\mathbf{x})}{1-P(y|\\mathbf{x})} = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p} = \\mathbf{w^Tx}\n",
    "\\end{equation}\n",
    "\n",
    "In practice,  we are really interested in the probability $P(y|\\mathbf{x})$ and not in the odds of $P(y|\\mathbf{x})$. To obtain this from the previous equation, we apply the *logistic function* (otherwise called a *sigmoid* function):\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^{z}}{1+e^{z}}\n",
    "\\end{equation}\n",
    "\n",
    "Applying this function on both sides of the LR equation, we get a model for $P(y|\\mathbf{x})$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y|\\mathbf{x}) = \\phi(\\mathbf{w^Tx}) = \\frac{1}{1 + e^{-\\mathbf{w^Tx}}}\n",
    "\\end{equation}\n",
    "\n",
    "If we want to classify a data point $\\mathbf{x}$, we can calculate the probability of it being class $y$ with the above equation. A final classification is then made by assigning it to the class if the probability $P(y|\\mathbf{x})$ exceeds a certain threshold. A typical threshold is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320607af-ec39-4868-a7c2-b7a04ad8d56a",
   "metadata": {},
   "source": [
    "![sigmoid](https://qph.fs.quoracdn.net/main-qimg-6ab7369356c16f17ac39fbb83d5d56c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f85d21-31d1-4877-897f-5b94e0a7ca6a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> THOUGHT EXERCISE: What would happen to our predictions when we would choose a lower threshold, let's say 0.2? How would this affect the accuracy of our predictions? Can you think of a situation where we would want to do this? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abab204-ed02-46de-8e26-20f5f77b7e21",
   "metadata": {},
   "source": [
    "#### 1.1.1 OPTIONAL EXTRA: Training a LR model: cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746f708-d301-4401-a783-bb3d29ce17ec",
   "metadata": {},
   "source": [
    "Now that we know the logistic regression model to predict the probability of belonging to a certain class, all that remains is the question of how to find the weights of the model on a given set of training data. As always, this is the problem of minimizing a loss function to find an optimal set of weights. Where we used the mean squared error (MSE) for linear regression, we will use the **cross-entropy** loss function for LR. Minimizing the binary cross-entropy loss is equivalent to minimizing the negative log-likelihood of the data under a binomial distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}-y_{i}log(P(y_i|\\mathbf{x}_i))-(1-y_i)log(1-P(y_i|\\mathbf{x}_i))\n",
    "\\end{equation}\n",
    "\n",
    "Where $y_i$ is the class of data point $i$ and $p(\\mathbf{x}_i)$ is the class-membership probability predicted by logistic regression for the observation $\\mathbf{x}_i$. If we look at the cross-entropy loss **for a single data point** $l_{log}^{i}$, we can break it down in two parts:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i} = \n",
    "\\begin{cases}\n",
    "   -log(P(y_i|\\mathbf{x}_i)) & \\text{if} \\ y_i = 1\\\\    \n",
    "   -log(1-P(y_i|\\mathbf{x}_i)) & \\text{if} \\ y_i = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Or even more generally, the cross-entropy can be written as the log probability of predicting the correct class:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i} = -log(P(\\mathrm{correct} y_i|\\mathbf{x}_i))\n",
    "\\end{equation}\n",
    "\n",
    "From this, it should be clear that the cross-entropy loss will be larger for smaller values of $P(y_i|\\mathbf{x}_i))$ if $y_i = 1$, and vice versa. If we visualize the cross-entropy loss for these two cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01daa3c2-772f-450c-8645-2ee910ac5de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "p = np.arange(0.01,0.99,0.01) # Generate a range of predicted probabilities between zero and 1\n",
    "l_0 = -np.log(p)  # cross-entropy loss if y = 1\n",
    "l_1 = -np.log(1-p)\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(p,l_0, marker='.')\n",
    "ax.scatter(p, l_1, marker='.')\n",
    "ax.set_xlabel('Predicted class-membership probability $P$')\n",
    "ax.set_ylabel('Cross-entropy loss')\n",
    "ax.legend(['Cross-entropy loss of a sample when y is 1',\n",
    "           'Cross-entropy loss of a sample when y is 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1142a-5cf6-4b52-816d-8aaa74fc7074",
   "metadata": {},
   "source": [
    "#### 1.1.2 OPTIONAL EXTRA: Training a LR model: gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65ec97-7e64-4e97-b40e-aa0473138c89",
   "metadata": {},
   "source": [
    "For linear regression, the solutions to the normal equations provide a convenient analytical solution to obtain the optimal set of model weights $\\mathbf{w}$ on a set of training data. There is no such solution to find the optimal weights for a logistic regression model, so instead an optimization algorithm such as **gradient descent** is used to train a LR model.\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that searches for the optimum of an objective function by making small changes to a set of optimization variables. Gradient descent (and its more complex variants) are widely used in machine learning to find the optimal set of model weights that minimize a certain loss function. Especially when there is no analytical solution for the weights available like for linear regression.\n",
    "\n",
    "Generally, gradient descent uses the **gradient of the loss function with respect to the model weights** to perform updates to those weights in each iteration. At iteration $k+1$, the algoritm computes the gradient of a loss function $J(\\mathbf{w})$ evaluated in the training data. Then, it performs an update to the current parameter values that is relative to the gradient multiplied with the learning rate $\\gamma$, which is a constant:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\gamma\\nabla{J(\\mathbf{w}_{k})}\n",
    "\\end{equation}\n",
    "\n",
    "Initially, the weights are often initialized with random draws from some distribution. The algorithm continues to do updates, until it converges or until some stopping criterion is reached.\n",
    "\n",
    "In order to perform gradient descent to find the weights of a logistic regression model, we need to compute the gradient of the loss function with respect to the model parameters. Recall that, for a single data point, the cross-entropy loss function was as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "l_{log}^{i}(\\mathbf{w}) = -y_{i}log(P(y_i|\\mathbf{x}_i))-(1-y_i)log(1-P(y_i|\\mathbf{x}_i)))\n",
    "\\end{equation}\n",
    "\n",
    "Where $P(y_i|\\mathbf{x}_i))$ is nothing else than the weighted sum of the inputs squashed through the sigmoid function:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_i|\\mathbf{x}_i)) = \\phi(w_{0}x_{0i} + w_{1}x_{1i} + ... + w_{p}x_{pi})\n",
    "\\end{equation}\n",
    "\n",
    "Before going on, let's first calculate the partial derivative of the sigmoid function:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial z} \\phi(z) = \\frac{\\partial}{\\partial z} \\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{(1+e^{-z})^2}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite this as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1 +e^{-z} -1}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} \\Big( 1 - \\frac{1}{1+e^{-z}}\\Big) = \\phi(z)(1 - \\phi(z))\n",
    "\\end{equation}\n",
    "\n",
    "With this result and by applying the chain rule, we can compute the partial derivative of the loss function with respect to the weight $w_j$. We will use the symbol $z$ to denote the weighted sum of the features (i.e., the input for the logistic function) and drop the subscript $i$ for clarity:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l_{log}(\\mathbf{w})}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\Big(-ylog(\\phi(z))-(1-y)log(1-\\phi(z)) \\Big) \\\\ = \\Big(  \\frac{-y}{\\phi(z)} + \\frac{1-y}{1-\\phi(z)}  \\Big)\\frac{\\partial}{\\partial w_j}\\phi(z) \\\\ = \\Big(  \\frac{-y}{\\phi(z)} + \\frac{1-y}{1-\\phi(z)}  \\Big) \\phi(z)(1-\\phi(z))\\frac{\\partial}{\\partial w_j}z\n",
    "\\end{equation}\n",
    "\n",
    "Since $z = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p}$, $\\frac{\\partial}{\\partial w_j}z$ is nothing more than $x_j$, so we can rewrite the above as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l_{log}(\\mathbf{w})}{\\partial w_j} = \\Big( -y(1-\\phi(z) + (1-y)\\phi(z))\\Big)x_j \\\\ = \\big(  -y + \\phi(z) \\big)x_j = \\big( \\phi(z) - y \\big)x_j\n",
    "\\end{equation}\n",
    "\n",
    "With this partial derivative of the loss w.r.t $w_j$, we can write the update rule of the gradient descent algorithm for the $j^{th}$ weight:\n",
    "    \n",
    "\\begin{equation}\n",
    "w_{j,k+1} = w_{j,k} - \\gamma(\\phi(z_k)-y)x_{j}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the algorithm will each time perform an update to the weight $w_{j}$ that is in proportion to the difference between the predicted probability of class membership in the previous iteration and the actual class. Makes sense! The entire gradient is simply the vector that contains the partial derivatives with respect to the entire weight vector $\\mathbf{w}$, and in reality gradient descent acts on $\\mathbf{w}$ and not on an individual weight $w_j$. Also, the gradient is typically not calculated for one data point, but evaluated over the entire training data set (unless the training set is prohibitively large, in that case mini-batch gradient descent is used). \n",
    "\n",
    "In practice, software packages such as [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegressionscikit-learn) do this optimization under the hood, so there is no need to implement it manually each time we want to use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2bc11-3f38-49c1-aa30-90730c5dae69",
   "metadata": {},
   "source": [
    "### 1.2 Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b6ee4-d219-4845-acdc-088a7fa170b5",
   "metadata": {},
   "source": [
    "In contrast to the (discriminative) logistic regression model, linear discriminative analysis (LDA) is a **generative** model. Generative models are called as they are because they specify the class-conditional densities $P(\\mathbf{x}|y)$, which allows to generate new observations $\\mathbf{x}$ for a class $y$: for a given class $y$, we can sample from the distribution $P(\\mathbf{x}|y)$. \n",
    "\n",
    "The main assumption in LDA is that the data in each class is assumed to follow a multivariate Gaussian distribution. Furthermore, the assumption is made that the (co)variance within and between predictor variables is the same for all the classes. As such, only a limited number of parameters have to be estimated from the data to fully specify the class-conditional distributions: in the multivariate case, we need to estimate the class-specific mean vectors $\\mathbf{\\mu_{k}}$ and the common covariance matrix $\\Sigma$. In the case of just one predictor (feature), this simplifies to estimating the mean of $x$ within each class and the variance $\\sigma^2$. The priors $p(y=k)$ are simply estimated from the data by calculating the proportion of training instances in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a882f-fc6c-42dc-835e-4655713e0678",
   "metadata": {},
   "source": [
    "![lda](https://d3i71xaburhd42.cloudfront.net/1ab8ea71fbef3b55b69e142897fadf43b3269463/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeabd4c-f0ec-485f-822c-86797c9395ce",
   "metadata": {},
   "source": [
    "Once the class-conditional densities $P(\\mathbf{x}|y)$ are modeled using LDA, predictions √† la logistic regression $P(y|\\mathbf{x})$ can be obtained by using Bayes' rule:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=k|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y=k)P(y)}{P(\\mathbf{x})} = \\frac{P(\\mathbf{x}|y=k)P(y=k)}{\\sum_{l=1}^{K}P(\\mathbf{x}|y=l)P(y=l)}\n",
    "\\end{equation}\n",
    "\n",
    "where all probabilities $P(\\mathbf{x}|y=l)$ is obtained by the LDA model and all probabilities $P(y=l)$ are simply observed from the class frequency in the dataset (e.g.: 70% of the training dataset samples belongs to class $l$ so: $P(y=l) = 0.70$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438f8c7-ca31-4617-a92f-cf23ce860d37",
   "metadata": {},
   "source": [
    "## 1.3 Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f8a4-06d5-4299-bcd2-1ec024e69ec3",
   "metadata": {},
   "source": [
    "As you may imagine, the classification accuracy (the percentage of samples predicted correctly), may not always be the most appropriate metric to evaluate the performance of a classifier. In particular, the following two scenarios seem to require a different approach: \n",
    "\n",
    "* In some applications, we want to avoid false negatives at the cost of more false positives. For instance, we want to make sure that we do not falsely predict that a patient is healthy when in reality he has a disease. When we allow for more false positives, the classification accuracy will go down. \n",
    "* When the data is very imbalanced, a *null classifier* that always predicts the same class will achieve a high classification accuracy.\n",
    "\n",
    "A first step to get some more insight in the performance of a classifier is to create a confusion matrix, which gives an overview of the kind of mistakes we have made during classification. A confusion matrix might look like this:\n",
    "\n",
    "![confusion](https://miro.medium.com/max/1000/1*fxiTNIgOyvAombPJx5KGeA.png)\n",
    "\n",
    "The concepts of *specificity* and *sensitivity* are better suited in the previous example scenarios. These are *class-specific* metrics: the sensitivity is the fraction of (true) positives that is correctly identified (as positive). The specificity is the fraction of (true negatives) that is correctly classified (as negative). A classifier might predict nearly every sample as positive, hence obtaining a high sensitivity but a very low specificity. One way to change this is to change the threshold for classification: by selecting a higher threshold, we will predict more samples as negatives, hence the sensitivity will go down because we will now predict some positives as being negative. However, the specificity will improve, because we now only predict samples as positive if the model outputs a higher probability for it being positive.\n",
    "\n",
    "![sensvsspec](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Sensitivity_and_specificity_1.01.svg/341px-Sensitivity_and_specificity_1.01.svg.png)\n",
    "\n",
    "The apparent trade-off between sensitivity and specificity is summarized by the receiver operating characteristic (ROC) curve. A ROC curve is a plot of the true positive rate (sensitivity) versus the false positive rate (one minus the specificity) for various thresholds. It typically looks like this:\n",
    "\n",
    "![ROC](https://miro.medium.com/max/724/1*eFmiqjRi5MmZwfaPsy82yg.png)\n",
    "\n",
    "It illustrates the trade-off between sensitivity and specificity as we vary the threshold. A good measure to evaluate a classifier is the **area under the ROC curve (AUROC or AUC)**. A classifier that does random guesses has an AUC of 0.5 (dotted line in the plot), so if we design a classifier for a certain problem we want it to achieve an AUC higher than 0.5. \n",
    "\n",
    "Check this [blog](https://datascienceplus.com/interpretation-of-the-auc/) to read a bit more considering the interpretation of the AUROC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76029b-4c52-4249-8118-4a1fe1431f25",
   "metadata": {},
   "source": [
    "## 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5fd492-9851-440b-b723-3a23a4155878",
   "metadata": {},
   "source": [
    "### 2.1 Breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a51876-fd4c-4160-8da9-ef52bf22a09a",
   "metadata": {},
   "source": [
    "In this set of exercises, we will use the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). The dataset contains information on the disease status of 569 breast cancer patients: they were either diagnosed with a malign (status M) or with a benign (status B) tumor. \n",
    "\n",
    "For each patient, the dataset also contains 30 features that represent statistics of the cell nuclei present in images taken after [fine needle aspirate tissue samples](https://en.wikipedia.org/wiki/Fine-needle_aspiration). These 30 features are the mean, standard deviation and the maximum of 10 measurements on the cell nuclei:\n",
    "- radius\n",
    "- texture\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness\n",
    "- compactness\n",
    "- concavity\n",
    "- concave points\n",
    "- symmetry\n",
    "- fractal dimension\n",
    "\n",
    "**Based on these feature of the cell nuclei, we would like to predict whether a patient has a malign or a benign breast cancer tumor.** Let's download and read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f31a68-5049-4b65-b07f-7fc83358f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/LinearClassification/wdbc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235338a5-837a-4f24-94df-39068166dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./wdbc.data', header=None, index_col=0, names=['Patient ID', 'status'] + list(np.arange(1,31,1)))\n",
    "status = data['status']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307da604-e41b-4a5e-b0cb-3529469f3b2e",
   "metadata": {},
   "source": [
    "First, let's look at the distribution of the disease status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd9c72-fdd5-4c75-a0b3-c3554db4ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data['status']).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317cbd1-51a6-40a9-bf34-8cfdf864d967",
   "metadata": {},
   "source": [
    "There are about 350 benign cases and roughly 200 malign cases. This is a fairly balanced dataset.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>THOUGHT EXERCISE: Suppose that the dataset was unbalanced, with 525 B cases and only 25 M cases. Would you still use accuracy to evaluate the model?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2e589-7e63-4f94-bf08-83391e546c5f",
   "metadata": {},
   "source": [
    "In these exercises, we will use the [scikit-learn](https://scikit-learn.org/stable/) implementations of [LR](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html). In order to fit models, we will first need to encode the labels 'Benign' (B) and 'Malign' (M) to a binary encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a2454-bec0-414a-b30d-5ad144e0e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder().fit(status)\n",
    "encoder.classes_  # 'B' will become class 0, 'M' will become class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b545166-f0b7-48eb-823b-cf33076a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.transform(status)\n",
    "x = data.drop('status', axis=1).values # Drop the disease status from the dataframe, convert to numpy array\n",
    "y[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e98a44-9d8f-48b3-a14b-17abec8fd1b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: Using scikit-learn, split the data in a 80% training and a 20% test set. Fit a logistic regression model and evaluate training and testing accuracy. </b>\n",
    "</div>\n",
    "\n",
    "Use [this method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train-test splitting and [this implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to perform logistic regression. Accuracy can be computed by using the score() method of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80b66b-3f70-468f-9418-517a6edce8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea370b71-a7a2-4825-a626-b995c6e407b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE: Now do the same with LDA. Do you split your dataset again in a new random training and testing split? Think about fair evaluation/comparison between the two models.</b>\n",
    "</div>\n",
    "\n",
    "Use [this implementation](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429fc4f-7b70-44c4-a314-7a690e1f82e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a65d05-4aa3-4174-b676-f536db0ce761",
   "metadata": {},
   "source": [
    "Depending on your data split, LDA or LR will perform better. The model that we prefer is hence determined by a random data splitting, a behavior that we do not want. One way to avoid this is to perform [cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/), we will return on this topic later in the course.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE: Use your preferred (LDA or LR) model to predict the class probabilities $P(y|\\mathbf{x})$ and the classes for the training data. Use the predict_proba() method to generate the predicted probabilities and .predict() to genereate the predicted classes. Take note of the output shape of predict_proba(), what do the columns mean? Use the code below to plot the two against each other. Which data points are most likely to be misclassified?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2fa93-d53e-4ef0-8e82-c267166a0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** solution **\n",
    "predicted_class_probabilities = ...\n",
    "predicted_classes = ...\n",
    "\n",
    "#** solution **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8c46f-a997-4ad2-bca2-f2a2c194108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "misclassified = predicted_classes !=  y_train\n",
    "\n",
    "colors = ['#b2182b' if wrong else '#2166ac' for wrong in misclassified ]\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "test_jitter = np.random.normal(scale=0.1, size=len(predicted_class_probabilities))\n",
    "ax.scatter(test_jitter, predicted_class_probabilities,\n",
    "           marker='.', s=25, color=colors, alpha=0.75)\n",
    "ax.get_xaxis().set_ticks([]);\n",
    "ax.set_ylabel('Predicted class probabilies').set_fontsize(20)\n",
    "ax.set_title('Correctly classified samples in blue', size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8c24b-3ddb-46f2-af34-86638ae6681e",
   "metadata": {},
   "source": [
    "Clearly, the misclassified points are those points where the predicted probability of class membership is rather close to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00303d-8cf2-4937-bd3e-a8bf7912d1da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE: Use the predicted class probabilities from the previous exercise to compute the ROC AUC score on the test dataset. Then, use the code below to plot the ROC curves of both models.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f7e51-7949-4b51-9b3c-74c0dceaedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curves\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Use test data!\n",
    "#########\n",
    "predicted_class_probabilities_LDA = ...\n",
    "predicted_class_probabilities_LR = ...\n",
    "\n",
    "AUC_LDA = roc_auc_score(..., ...)\n",
    "AUC_LR = roc_auc_score(..., ...)\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb6659-5441-47e1-a6bc-e0c5803b1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_LDA = roc_curve(y_test, predicted_class_probabilities_LDA)\n",
    "roc_LR = roc_curve(y_test, predicted_class_probabilities_LR)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.plot([0, 1], [0, 1], '--')\n",
    "ax.plot(roc_LR[0], roc_LR[1])\n",
    "ax.plot(roc_LDA[0], roc_LDA[1])\n",
    "ax.set_xlabel('1-specificity').set_fontsize(18)\n",
    "ax.set_ylabel('sensitivity').set_fontsize(18)\n",
    "\n",
    "\n",
    "ax.legend(['Random classifier - AUC 0.5',\n",
    "           'LR - test data AUC {}'.format(AUC_LR),\n",
    "           'LDA - test data AUC {}'.format(AUC_LDA)],\n",
    "         fontsize='large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f880743-220e-4914-93e6-f789aafaa76a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b> EXERCISE: \n",
    "We have succesfully implemented a pipeline going from data exploration to model construction and evaluation (including comparison) for the wisconsin breast cancer dataset. For the end of the PC lab, we introduce a new dataset: [Job change analytics](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists). Implement the data exploration and model construction pipeline for this dataset by reusing code from previous exercises. The goal of this exercise is to show that every dataset has its own unique set of characteristics and challenges.</b>\n",
    "</div>\n",
    "\n",
    "Some information about the dataset: A company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates really want to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment. **Hence, the input features are details about persons who took data science courses, and the output of interest (target) is whether those persons are actually looking for a new job.**\n",
    "\n",
    "More information [here](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists).\n",
    "\n",
    "- What do you do with missing values in the dataset?: You can throw samples with at least one missing feature value out, or you can [impute](https://scikit-learn.org/stable/modules/impute.html) those missing values.\n",
    "- What about class imbalance?\n",
    "- Many features are categorical, how will you encode these?\n",
    "\n",
    "Since there is no single right way to answer the above questions, we will not provide a solution to this exercise. Experiment at free will! If you have any questions, you can always mail the teaching assistant.\n",
    "\n",
    "Below is some <u>preliminary</u> data exploration. Use this to further explore the data and make design choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c85208-0a73-4e4a-822a-b7a15eb4a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/LinearClassification/HR_analytics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1d814-8a0c-4f58-adc3-a65cd9c17a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HR_analytics.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0374e-a571-4678-bb4b-4e47531db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ba526-b5ca-4e33-9ab7-bc728e257781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of NaN (missing values) per feature:\n",
    "pd.isnull(data).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fba2b-fa83-44aa-8128-044e7cfdcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values per feature:\n",
    "for k in data.columns:\n",
    "    print(k, len(np.unique(data[k][~pd.isnull(data[k])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14622b59-b400-4a9d-a591-954ac1729dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what the unique values are for a specific feature:\n",
    "feature = 'enrolled_university'\n",
    "np.unique(data[feature][~pd.isnull(data[feature])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e56df-0e00-4ff1-bf38-a8f2bccbf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### YOUR OWN CODE HERE #######################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff0d95-b9ce-4202-9990-3f43d982d3ce",
   "metadata": {},
   "source": [
    "### 2.2 OPTIONAL EXTRA: Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74e505-4b14-42bf-97f4-9cbdf2d8acb4",
   "metadata": {},
   "source": [
    "Some classifiers inherently support multiclass classification as part of their design, for example, in LDA, the class-conditional density $P(\\mathbf{x}|y)$ is modeled for every class $y$ and can be converted to predictions using Bayes' rule. For logistic regression, a natural extension to the multi-class case is [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression). Check out the [scikit-learn page on multiclass prediction](https://scikit-learn.org/stable/modules/multiclass.html) for more information. In principle, any classifier can be made multiclass using following methods:\n",
    "\n",
    "#### One-versus-one classification\n",
    "\n",
    "One-versus-one classification is another approach to a multiclass classification problem. For a K-class problem, the strategy consists of training $\\frac{K(K-1)}{2}$ classifiers. Each of these classifiers much learn to distinguish to classes. One the classifiers are trained, a voting scheme is applied to make a prediction for an unseen data point: each classifier has to decide between two possible classes. The final predicted class is that class that gets the largest number of votes. \n",
    "\n",
    "#### One-versus-all classification\n",
    "\n",
    "In one-versus-all (OvA) classification (also called one-versus-rest), a single classifier is trained per class, with the samples of that class as positive samples and all other samples as negatives. The strategy proceeds as follows for a K-class classification problem:\n",
    "\n",
    "**Inputs:**\n",
    "* a classification algorithm L (learner)\n",
    "* feature matrix $\\mathbf{X}$\n",
    "* label vector y where $y_i \\in {1,...,K}$\n",
    "\n",
    "\n",
    "**Procedure:**\n",
    "for each k in {1,...,K}:\n",
    "* construct a new label vector z where $z_i$ is 1 if $y_i$ = k and 0 otherwise\n",
    "* train L on $\\mathbf{X}$ to obtain a classifier $f_k$. The classifier should return class probabilities and not hard labels.\n",
    "\n",
    "**Returns**\n",
    "A list of trained classifiers $f_k$ for each  k in {1,...,K}\n",
    "\n",
    "To make predictions for a new sample $\\mathbf{x}$, the $k$ classifiers are applied to $\\mathbf{x}$ and the final predicted label is the label that is predicted with the highest confidence (probability):\n",
    "\n",
    "$\\hat{y} = \\underset{k \\in {1,...,K}}{\\mathrm{argmax}} \\, f_k(\\mathbf{x})$\n",
    "\n",
    "Let's simulate a toy dataset with three classes and two features, and split it in training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb57c6-a91d-445e-a107-440e68c180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers= [[-2.5, 0], [0, 1], [3.5, -1]], random_state=42)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306093e-50fc-45e2-93bb-50a224436eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "colors=['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "for i, color in enumerate(colors):\n",
    "    idx_train = np.where(y_train==i)\n",
    "    idx_test = np.where(y_test==i)\n",
    "    plt.scatter(X_train[idx_train,0], X_train[idx_train,1], c=color, edgecolor='black', s=30)\n",
    "    plt.scatter(X_test[idx_test,0], X_test[idx_test, 1],c='white', edgecolor=color, s=70)\n",
    "    \n",
    "ax.legend(['Class 1 - train',\n",
    "           'Class 1 - test',\n",
    "           'Class 2 - train',\n",
    "           'Class 2 - test',\n",
    "           'Class 3 - train',\n",
    "           'Class 3 - test']);\n",
    "\n",
    "ax.set_xlabel('Feature 1');\n",
    "ax.set_ylabel('Feature 2');\n",
    "ax.set_title('Toy dataset for multiclass classification').set_fontsize(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df44bb7-77de-45df-b4af-1a8d0496bec2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>OPTIONAL EXTRA EXERCISE: Look again at the documentation of LDA and Logistic Regression. What options do you need to specify to train these models on a multi-class problem? Train a model on this toy dataset</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f94238-9c1d-4b00-89c4-eb9c809dc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### exercise start ############\n",
    "\n",
    "# instantiate model ...\n",
    "\n",
    "# fit ...\n",
    "\n",
    "predicted_classes = ...\n",
    "\n",
    "classification_accuracy = np.round(np.mean(y_test == predicted_classes)*100,2)\n",
    "\n",
    "######### exercise stop ############\n",
    "\n",
    "# Visualize the predictions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "colors=['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "\n",
    "for i, color in enumerate(colors):\n",
    "    idx_train = np.where(y_train==i)\n",
    "    idx_test = np.where(y_test==i)\n",
    "    \n",
    "    plt.scatter(X_train[idx_train,0], X_train[idx_train,1], c=color, edgecolor='black', s=30)\n",
    "    plt.scatter(X_test[idx_test,0], X_test[idx_test, 1],c='white', edgecolor=color, s=70)\n",
    "        \n",
    "ax.legend(['Class 1 - train',\n",
    "           'Class 1 - test',\n",
    "           'Class 2 - train',\n",
    "           'Class 2 - test',\n",
    "           'Class 3 - train',\n",
    "           'Class 3 - test']);\n",
    "\n",
    "# add predictions\n",
    "for i, color in enumerate(colors):\n",
    "    idx_predicted = np.where(predicted_classes==i)\n",
    "    plt.scatter(X_test[idx_predicted,0], X_test[idx_predicted,1], c=color, marker='s', s=2)\n",
    "\n",
    "ax.set_xlabel('Feature 1');\n",
    "ax.set_ylabel('Feature 2');\n",
    "ax.set_title('Toy dataset for multiclass classification - classification accuracy: {}%'.format(classification_accuracy)).set_fontsize(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb1559-d45e-4f22-bf89-208c875360f0",
   "metadata": {},
   "source": [
    "### 2.3 OPTIONAL EXTRA: Data preprocessing for text inputs (example with DNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5c0db-3741-4312-9436-d37586de15cc",
   "metadata": {},
   "source": [
    "Since machine learning models expect numbers as inputs, not words, we need to do some preprocessing first to make them compatible with ML models. A very common feature representation for text sequences is called the **Bag of Words**. It consists of listing all the possible tokens that might occur in your text. A token might be one word, or a combination of two or multiple words. The feature representation is then a matrix with the count of the tokens for each text instance. The simplest transformation is to just look at the counts of individual tokens: the 1-grams. However, it might be interesting to count combinations of two or more words as well. Consider the following example, where we have 2 example input sample sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2687a32-aa48-47ac-85e8-d9ca871edc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = ['this is real life',\n",
    "       'is this real life']\n",
    "print('data:\\n', data[0], '\\n', data[1])\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
    "\n",
    "analyze = vectorizer.build_analyzer()\n",
    "print('vocabulary:\\n', analyze(data[0]))\n",
    "      \n",
    "features = vectorizer.fit_transform(data)\n",
    "print('final feature representation:\\n', features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76defec4-5339-47e5-8f90-eee4e7a88d57",
   "metadata": {},
   "source": [
    "Note that by default, the vectorizer returns the features as a sparse array: in most applications, each row of the feature matrix will contain a lot of zeros. For this exercise, you can convert it to a regular numpy array with the ```toarray()``` method.\n",
    "\n",
    "If we restrict ourselves to look at 1-grams, both sentences above are transformed into the exact same feature representation. However, suppose we want to do sentiment analysis and classify whether a sentence contains a question or not. Then it might be useful to look at bigrams as well, as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77830cc6-1c26-435b-a975-64c612dc6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(data[0])\n",
    "features = vectorizer.fit_transform(data)\n",
    "print('Only 1-grams:')\n",
    "print('final feature representation:\\n', features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0603f-e6a3-42cc-9890-3e9e55406309",
   "metadata": {},
   "source": [
    "When we consider bigrams, the feature representation for both sentences is no longer exactly the same. Maybe the features that represent 'this is' and 'is this' could be helpful here.\n",
    "\n",
    "Still the bag-of-words features completely ignores where every n-gram was in the sentence. Another way to take this into account is to assign $v$ dummy features for every location in the sentence, with $v$ being the vocabulary size (number of possible different words). Every first, second, third and fourth word get assigned 1 feature for every possible word in that location (out of 4 possible words in the vocabulary). This is called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Natural_language_processing)\n",
    "This requires the sentences to be all of the same length, for example, 4 words as in the 2 sentences above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f472098-d002-4d8e-8242-2a6f76819d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splitwords = [sentence.split(' ') for sentence in data]\n",
    "print('data splitted by words:')\n",
    "print(data_splitwords)\n",
    "vocab = list(set([word for sentence in data_splitwords for word in sentence]))\n",
    "print('vocabulary:')\n",
    "print(vocab)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore',\n",
    "                    categories=[vocab]*len(data_splitwords[0]))\n",
    "\n",
    "features = enc.fit_transform(data_splitwords)\n",
    "print('resulting feature representation:')\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128da97a-e5b5-4361-8220-93587b8b4f11",
   "metadata": {},
   "source": [
    "If it is not immediately clear what the features mean in this example: the first four features represent which word is the first word in each sentence. The first and sentence has as first word 'this', this is the last word in the vocabulary, so of the first four features, the last is given by $1$. The second sentence has 'is' as the first word, which is the second word in the vocabulary, for this reason, the second column of the first four features is given by $1$ for this sentence. Both sentences have the same last two words 'real life', it can be seen that the last 8 features for both sentences are identical.\n",
    "\n",
    "Armed with these encoding schemes, we can tackle a real problem. **DNA sequences** as inputs are peculiar because they are essentially text: following an alphabet containing only A, T, C or G. We will use data from a published [study](https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-12-4) about sigma factors in E. coli. Some biological background information for those interested:\n",
    "\n",
    "> The binding region of the transcription unit,  commonly called the promoter region, is known to play a key role  in the transcription rate of downstream genes. In Eubacteria, the sigma factor ($\\sigma$) binds with the RNA  polymerase subunit ($\\alpha\\beta\\beta'\\omega$) to create RNA polymerase. Being part of the RNA polymerase holoenzyme, the sigma element acts as the connection between RNA polymerase and DNA. Depending on the sigma factor bound to the transcription unit, different binding regions of the enzyme with the DNA are observed. The data represents specific DNA regions of the *E. coli* genome to which one or more sigma factors bind.\n",
    "\n",
    "In less-biological-jargon words: certain molecules called sigma factors control activity of genes, they do so by binding to the DNA close to the gene. If we can predict for an input DNA sequence if such a molecule will bind, we gain an understanding in the biological function of those genes and molecules.\n",
    "\n",
    "The dataset contains 4724 DNA sequences. For each sequence, we have information on five target variables, each representing 5 sigma factors. Each time, the outcome is binary, indicating whether the sigma factor binds to the sequence or not. This is a **multi-label** problem, where an input can belong to any (multiple at once) of the classes, as opposed to only one possible class for each sample in multi-class. For simplicity in this exercise, we will focus only on one class: RPOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e920919-d888-41f4-a55a-486938fe3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/LinearClassification/promotor_list_exp_growth.csv\n",
    "promotor_data = pd.read_csv('./promotor_list_exp_growth.csv', index_col=0)\n",
    "promotor_data.head(20)\n",
    "\n",
    "X = np.array(promotor_data.PROBE_SEQUENCE)\n",
    "y = promotor_data['RPOD'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816f3c3-2965-4997-a655-0e77b4971e02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b> OPTIONAL EXTRA EXERCISE: \n",
    "Use one of the encoding schemes to preprocess the DNA data. Construct a model and evaluate performance on the test set. Play around with different preprocessing values, and keep testing performances. What types of preprocessing lead to good performance for this dataset? In what cases are we overfitting? Think about fair evaluation: if we keep testing out configurations and evaluating on the test set, is our final test performance estimate fair? We can solve this by keeping an additional data split separate: training, validation and testing. Then we test different hyperparameters (different ways of preprocessing) on the validation set. We don't touch the test set until we have our final model. This is the only fair way of evaluating. For more information, see this [link](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).</b>\n",
    "</div></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1b6d7-d837-48ba-b3af-41e85990e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### exercise start ############\n",
    "\n",
    "\n",
    "# instantiate encoder\n",
    "\n",
    "# fit and transform data with encoder\n",
    "\n",
    "# instantiate modle\n",
    "\n",
    "# fit model\n",
    "\n",
    "# evaluate\n",
    "\n",
    "######### exercise stop ############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
