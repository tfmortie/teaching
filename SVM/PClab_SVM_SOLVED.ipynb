{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are (still) among the most popular techniques used for classification and regression. As these methods only use instances as products of the feature vectors, they can be kernelized by means of the kernel trick:\n",
    "\\begin{align}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) &= \\phi(\\mathbf{x_i})^T\\phi(\\mathbf{x_j}). \n",
    "\\end{align}\n",
    "\n",
    "SVMs are what people call a _maximum margin classifier_, as it can be shown that the mathematical framework of an SVM maximizes the distance between the decision boundary and the instances closest to the boundary. In addition, they are considered sparse methods because after the training phase a part of the data set can usually be discarded, retaining only the _support vectors_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![margin](https://images.squarespace-cdn.com/content/v1/5d782753c70af105c29a9b14/1579546690818-IWIFF5WUEPMS3068E9G9/Screen+Shot+2020-01-20+at+1.57.48+PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, the SVM tries to maximize the margin $M$, while retaining an optimal classification error. As is shown in theory lecture, the first is equal to $||\\mathbf{w}||^{-1}$ and so this is equivalent to minimizing: \n",
    "\\begin{equation}\n",
    "    \\text{arg min} \\frac{1}{2} ||\\mathbf{w} + w_0||^2, \n",
    "\\end{equation}\n",
    "\n",
    "subject to: \n",
    "\\begin{equation}\n",
    "    y_i(\\mathbf{x}^T(\\mathbf{w} + w_0)) \\geq 1, \\qquad \\forall i =  1, ..., n. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this implementation only works for data that is separable. By allowing misclassifications one can turn the margin classifier into the well-known SVM by introducing slack variables $\\xi_i$: \n",
    "\\begin{equation}\n",
    "    \\text{arg min}_{\\mathbf{w} + w_0} \\{ \\frac{1}{2} ||\\mathbf{w} + w_0||^2 + C \\sum_{i=1}^n \\xi_i \\}, \n",
    "\\end{equation}\n",
    "\n",
    "subject to: \n",
    "\\begin{equation}\n",
    "    y_i(\\mathbf{x}^T(\\mathbf{w}+w_0)) \\geq 1-\\xi_i, \\qquad \\forall i =  1, ..., n. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formulation can be translated to an equivalent _dual representation_, which leads to the following maximization problem:\n",
    "\\begin{equation}\n",
    "    \\text{arg max}_{a_i} \\{ \\sum_{i=1}^n a_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{k=1}^n a_i a_k y_i y_k \\mathbf{x_i}^{T}\\mathbf{x_k} \\}, \n",
    "\\end{equation}\n",
    "\n",
    "subject to: \n",
    "\\begin{align}\n",
    "    0 \\leq a_i &\\leq C, \\\\\n",
    "    \\sum_{i=1}^n a_iy_i &= 0, \\qquad \\forall i = 1, ..., n, \n",
    "\\end{align}\n",
    "\n",
    "Due to the dot products in the above formulation, we can apply the **kernel trick**, which then gives us the final maximization problem:\n",
    "\\begin{equation}\n",
    "    \\text{arg max}_{a_i} \\{ \\sum_{i=1}^n a_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{k=1}^n a_i a_k y_i y_k k(\\mathbf{x_i},\\mathbf{x_k}) \\}, \n",
    "\\end{equation}\n",
    "\n",
    "subject to: \n",
    "\\begin{align}\n",
    "    0 \\leq a_i &\\leq C, \\\\\n",
    "    \\sum_{i=1}^n a_iy_i &= 0, \\qquad \\forall i = 1, ..., n, \n",
    "\\end{align}\n",
    "\n",
    "where $k$ is defined as the kernel function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PC-lab we will use SVMs for both classification and regression purposes. In the first part we will predict the label of two bacterial species  based on flow cytometry data. Next, we will try to predict the molecular function of a set of proteins by using only its amino acid sequence by using a customized spectral kernel. Finally, support vector regression (SVR) will be used to have a second attempt at the prediction of the strength of concrete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search vs. randomized grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid](https://miro.medium.com/proxy/1*ZTlQm_WRcrNqL-nLnx6GJA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs typically contain multiple hyperparameters. First of all there is the penalty of regularization (often noted as $C$ for SVMs, equivalent to $1/\\lambda$ previously seen in theory). Second there is a kernel to choose, and depending on the kernel, there can be one (or sometimes more) hyperparameters to tune (often denoted with a $\\gamma$). One can even include the type of kernel as a hyperparameter. It is straightforward to note that the computational time of training a model increases exponentially with the number of hyperparameters that needs to be  tuned. \n",
    "\n",
    "Two searches are often proposed to tune a machine learning model in function of its hyperparameters: \n",
    "- the first considers an exhaustive and structured search in the full hyperparameter space; \n",
    "- the second applies random combinations of hyperparameter settings. \n",
    "\n",
    "[It has been shown that when only few hyperparameters have an impact on model performance a randomized grid search is preferred. ](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) \n",
    "\n",
    "`Scikit-learn` offers implementations of [both](http://scikit-learn.org/stable/modules/grid_search.html), of which we will compare the use in this PC-lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classification of microbial flow cytometry data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a second look at the flow cytometry dataset. The 'SC' features measure scatter, and say something about the morphologhy of the cells (FSC: forward scatter, SSC: sideway scatter). The 'FL' features are fluorescence features from different parts of the spectrum. Two bacterial species have been measured separately, _Shewanella oneidensis_ and _Sphingomonas aromaticivorans_, labelled 1 and 0 respectively. Scikit-learn provides [various implementations of SVMs](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm), which we will use to analyze this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 1: \n",
    "<br>\n",
    "a) Read in the data from the files `fc_train` and `fc_test`. Often, before flow cytometry data can be analyzed, mathematical transformations are applied to for example visualize the data. Therefore we first need to preprocess the data by performing an $\\text{arcsinh}(x)$ transformation and normalization (or standardization). Why do we need to normalize for SVMs? Sample 10% of the data (due to long runtimes) and perform the suggested preprocessing steps. \n",
    "<br>\n",
    "b) Fit and optimize a SVM with a linear kernel to the training data. The 'species' label denotes the target to predict, you can ignore the 'Width' and 'Time' feature. Evaluate the performance on the test set. What's the 'optimal' performance of this model? Which hyperparameter setting goes along with this? What does this mean? \n",
    "<br>\n",
    "c) Explore other kernels to tackle this dataset. Find optimal hyperparameters using a grid search. \n",
    "<br>\n",
    "d) Another possibility is to use a randomized grid search. How long did previous exercise take? How long does a randomized grid search take before you reach the same performance? What do you prefer? \n",
    "<br>\n",
    "e) What would be an alternative option to tune your model? \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a): \n",
    "\n",
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/SVM/fc_train.csv\n",
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/SVM/fc_test.csv\n",
    "\n",
    "#Tip: np.arcsinh()\n",
    "\n",
    "fc_train = pd.read_csv('fc_train.csv', header=0, index_col=0)\n",
    "fc_test = pd.read_csv('fc_test.csv', header=0, index_col=0)\n",
    "fc_train = fc_train.sample(frac=.1, weights='species')\n",
    "features = fc_train.columns[:-3]\n",
    "display(fc_train.head(5))\n",
    "\n",
    "fc_train_trans = np.arcsinh(fc_train[features])\n",
    "fc_test_trans = np.arcsinh(fc_test[features])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fc_train_trans)\n",
    "fc_train_scaled = scaler.transform(fc_train_trans)\n",
    "fc_test_scaled = scaler.transform(fc_test_trans)\n",
    "\n",
    "fc_train_scaled = pd.DataFrame(fc_train_scaled, columns=features)\n",
    "fc_test_scaled = pd.DataFrame(fc_test_scaled, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fc_test_scaled.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): \n",
    "acc = np.zeros(10)\n",
    "i=0\n",
    "for c in np.logspace(-4,4,10): \n",
    "    svc = SVC(C=c,kernel='linear')\n",
    "    svc.fit(fc_train_scaled[features],fc_train['species'])\n",
    "    y_pred = svc.predict(fc_test_scaled[features])\n",
    "    acc[i] = accuracy_score(fc_test['species'],y_pred)\n",
    "    i+=1\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.logspace(-4,4,10), acc, c='r')\n",
    "ax.set_xlabel('C', size=18)\n",
    "ax.set_ylabel('Accuracy', size=18)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(10**(-5),10**5)\n",
    "ax.set_ylim(0.75,0.95)\n",
    "plt.show()\n",
    "\n",
    "print('Performance Linear SVM: ' + str(acc.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##1c): \n",
    "svc_gs = GridSearchCV(SVC(kernel='rbf'), param_grid={\"C\": np.logspace(-3,3,10), \"gamma\": np.logspace(-2, 2, 10)}, cv=3)\n",
    "svc_gs.fit(fc_train_scaled[features],fc_train['species'])\n",
    "y_pred = svc_gs.predict(fc_test_scaled[features])\n",
    "acc_gs = accuracy_score(fc_test['species'],y_pred)\n",
    "print('Accuracy SVM with rbf-kernel after grid search: ' + str(acc_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#1d):\n",
    "svc_rs = RandomizedSearchCV(SVC(kernel='rbf'), param_distributions={\"C\": np.logspace(-3,3,1000), \"gamma\": np.logspace(-3, 3, 1000)}, n_iter=50, cv=3)\n",
    "svc_rs.fit(fc_train_scaled[features],fc_train['species'])\n",
    "y_pred = svc_rs.predict(fc_test_scaled[features])\n",
    "acc_rs = accuracy_score(fc_test['species'],y_pred)\n",
    "\n",
    "print('Accuracy SVM with rbf-kernel after randomized grid search: ' + str(acc_rs))\n",
    "svc_rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines with custom kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only requirement for a valid kernel is that it is positive definite and symmetric. Therefore, it is possible to define a kernel function over complex structures, making kernel methods particularly useful in bioinformatics,\n",
    "where the instances are often sequences, graphs, trees, molecules or texts, rather than vectors of fixed length. However, the use of these types of kernels can be computationally demanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part of the PC-lab we will use a spectrum kernel for predicting the molecular function of a set of protein sequences. This kernel is based on the number of $k$-mers that two sequences have in common. Suppose that each sequence is denoted as $\\mathbf{x_i} \\in \\Sigma^∗$ where $\\Sigma$ is the alphabet of amino acids or nucleotides, depending on the application. A $k$-mer $\\mathbf{a} \\in \\Sigma_k$ is a sequence of length $k$. The sequence $\\mathbf{x_i}$ contains $\\mathbf{a}$ iff $\\mathbf{x_i} = \\mathbf{uav}$. Let $N(\\mathbf{a}, \\mathbf{x_i})$ be the number of times $\\mathbf{a}$ appears in sequence $\\mathbf{x_i}$. With this notation in mind, the _spectrum kernel function_ between two strings $\\mathbf{x_i}$ and $\\mathbf{x_j}$ is defined as:\n",
    "\\begin{equation}\n",
    "    SK_k(\\mathbf{x_i},\\mathbf{x_j}) = \\sum_{\\mathbf{a} \\in \\Sigma_k} N(\\mathbf{a},\\mathbf{x_i}) \\times N(\\mathbf{a},\\mathbf{x_j}). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is biased toward sequences that contain multiple instances of the same k-mer. This can easily be seen when considering a pair of sequences that contains the same $k$-mer twice. Such a pair gains a score of 4, while having two different $k$-mers in common only gives a score of 2. For this reason a normalized spectrum kernel can be used as alternative, defined as:\n",
    "\\begin{equation}\n",
    "    NSK_k(\\mathbf{x_i},\\mathbf{x_j}) = \\sum_{\\mathbf{a} \\in \\Sigma_k} \\text{min}\\left( N(\\mathbf{a},\\mathbf{x_i}),N(\\mathbf{a},\\mathbf{x_j})\\right). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized kernel has an explicitly defined mapping $\\phi(\\mathbf{x})$. If the maximum length of the sequences is $n$, then $\\phi(\\mathbf{x})$ defines a map of $\\mathbf{x} \\in \\Sigma^n$ to a feature space of dimension $|\\Sigma^kn|$, where each dimension is indexed by a $k$-mer a and an integer $1 ≥ i ≥ n$. The mapping is as follows:\n",
    "\\begin{equation}\n",
    "\\phi_{(\\mathbf{a},i)}(\\mathbf{x}) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\text{if $k$-mer $\\mathbf{a}$ appears at least $i$ times in $\\mathbf{x}$}, \\\\\n",
    "        0 & \\text{otherwise}. \n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these two kernels, we will analyze 150 protein sequences and predict their functional label. Each molecule belongs either to the category `transport`, `glycosylation` or `signaling`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Calculate (Normalized) Spectrum Kernel`: \n",
    "\n",
    "**Input**: Sequence 1, Sequence 2, $k$; \n",
    "\n",
    "**1. ** Find all possible substrings of length $k$ in sequence 1. \n",
    "\n",
    "**2. ** Find all possible substrings of length $k$ in sequence 2.\n",
    "\n",
    "**3. ** Find common subset of substrings present in sequence 1 and sequence 2. \n",
    "\n",
    "**4. ** Count the number of times a specific substring is present in sequence 1 and 2 and calculate $SK$(sequence1,sequence2) or $NSK$(sequence1,sequence2) according to the formulas above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 2: \n",
    "<br>\n",
    "In attachment to this PC-lab, you can find two files called `Spectrum_Kernel.csv` and `Spectrum_Kernel_norm.csv`. The labels of the proteins can be found in the file `ProteinFunctionalLabels.txt`. These files contain the gram matrices generated according to the code above. \n",
    "<br>\n",
    "a) What kind of classification problem is this? How can this problem be approached? \n",
    "<br>\n",
    "b) Which kernel representation works better? Evaluate the spectrum kernels using a SVM. Use 70% of the data for training and 30% for testing. Think about the properties of your kernel matrices. How should those training and test sets be constructed? How should the kernel matrix be feeded to the SVM? Don't forget to tune the complexity parameter $C$. \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/SVM/Spectrum_Kernel.csv\n",
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/SVM/Spectrum_Kernel_norm.csv\n",
    "\n",
    "#Some preprocessing\n",
    "\n",
    "spectrum_kernel = pd.read_csv('Spectrum_Kernel.csv', header=None, index_col=None)\n",
    "spectrum_kernel_norm = pd.read_csv('Spectrum_Kernel_norm.csv', header=None, index_col=None)\n",
    "labels = pd.read_table('ProteinFunctionalLabels.txt',header=None, index_col=None)\n",
    " \n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "y = pd.DataFrame(y,dtype=int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrum_kernel, y, train_size=0.7, stratify=y, random_state=5)\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(spectrum_kernel_norm, y, train_size=0.7, stratify=y, random_state=5)\n",
    "X_train = X_train.iloc[:,X_train.index]\n",
    "X_test = X_test.iloc[:,X_train.index]\n",
    "X_train_norm = X_train_norm.iloc[:,X_train_norm.index]\n",
    "X_test_norm = X_test_norm.iloc[:,X_train_norm.index]\n",
    "\n",
    "display(X_train.head(5))\n",
    "display(X_train_norm.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros(50)\n",
    "i=0\n",
    "for c in np.logspace(-4,-1,50): \n",
    "    svc = SVC(C=c,kernel='precomputed',max_iter=500)\n",
    "    svc.fit(X_train,y_train.values.ravel())\n",
    "    y_pred = svc.predict(X_test)\n",
    "    acc[i] = accuracy_score(y_test,y_pred)\n",
    "    i+=1\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.logspace(-4,-1,50), acc, c='r')\n",
    "ax.set_xlabel('C', size=18)\n",
    "ax.set_ylabel('Accuracy', size=18)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(10**(-5),10**0)\n",
    "ax.set_ylim(0.1,1)\n",
    "plt.show()\n",
    "\n",
    "print('Protein label identification performance using Spectrum Kernel: ' + str(acc.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=0.001,kernel='precomputed',max_iter=1000)\n",
    "svc.fit(X_train,y_train.values.ravel())\n",
    "y_pred = svc.predict(X_test)\n",
    "confusionmatrix = confusion_matrix(y_test.values.ravel(),y_pred)\n",
    "display(confusionmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_norm = np.zeros(50)\n",
    "i=0\n",
    "for c in np.logspace(-4,-1,50): \n",
    "    svc = SVC(C=c,kernel='precomputed',max_iter=1000)\n",
    "    svc.fit(X_train_norm,y_train_norm.values.ravel())\n",
    "    y_pred = svc.predict(X_test_norm)\n",
    "    acc_norm[i] = accuracy_score(y_test_norm,y_pred)\n",
    "    i+=1\n",
    "    \n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.logspace(-4,-1,50), acc_norm, c='r')\n",
    "ax.set_xlabel('C', size=18)\n",
    "ax.set_ylabel('Accuracy', size=18)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(10**(-5),10**0)\n",
    "ax.set_ylim(0.1,1)\n",
    "plt.show()\n",
    "\n",
    "print('Protein label identification performance using Normalized Spectrum Kernel: ' + str(acc_norm.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=0.001,kernel='precomputed',max_iter=1000)\n",
    "svc.fit(X_train_norm,y_train_norm.values.ravel())\n",
    "y_pred = svc.predict(X_test_norm)\n",
    "confusion_matrix_norm = confusion_matrix(y_test_norm, y_pred)\n",
    "display(confusion_matrix_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs can also be adapted to be used for regression problems, using a so-called _$\\epsilon$-insensitive error function_, which returns a zero error if the absolute difference between the prediction and target is less than $\\epsilon$, with $\\epsilon > 0$. If we note predictions for instance $i$ as $y_i^*$, we can write the error functions as follows: \n",
    "\\begin{align}\n",
    "E_\\epsilon(y_i - y_i^*) &= 0, \\quad \\text{if} \\quad |y_i - y_i^*| < \\epsilon, \\\\\n",
    "E_\\epsilon(y_i - y_i^*) &=  |y_i - y_i^*| - \\epsilon, \\quad \\text{otherwise}. \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now need two kinds of slack variables (why is that?), the mathematical optimization scheme now looks as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{arg min}_{a_i,a_i^*} \\{ \\epsilon\\sum_{i=1}^n (a_i+a_i^*) -  \\frac{1}{2}\\sum_{i=1}^n \\sum_{k=1}^n (a_i-a_i^*) (a_k-a_k^*) K + \\sum_{i=1}^n (a_i - a_i^*)y_i \\} , \n",
    "\\end{equation}\n",
    "\n",
    "subject to: \n",
    "\\begin{align}\n",
    "    0 &\\leq a_i, \\\\\n",
    "    a_i^* &\\leq C, \\\\\n",
    "    \\sum_{i=1}^n (a_i - a_i^*) &= 0, \\\\\n",
    "    a_ia_i^* &= 0, \\qquad \\forall i = 1, ..., n.   \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative formulations of SVMs are also possible. One of them are called $\\nu$-SVMs, where the amount of support vectors $\\nu$ are controlled. Those interested can have a look [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 3 (Optional): \n",
    "<br>\n",
    "We will have a second look at the `Concrete Compressive Strength` dataset. We are going to apply a [support vector regression model](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) to the dataset. \n",
    "<br>\n",
    "a) Which hyperparameters do you have in this setting? How would you optimize the model? \n",
    "<br>\n",
    "b) Fit a support vector regression model to the data. Start with the rbf kernel. Evaluate your performance on a 30% held-out test set in terms of the mean squared error (MSE). Can you do better than kernel ridge regression? If the training takes too long, try to use a subsample of the data. \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/tfmortie/teaching/main/LinearClassification/concreteComprStrength.txt\n",
    "\n",
    "df = pd.read_table('concreteComprStrength.txt', delim_whitespace=True, header=0, index_col=None)\n",
    "df = df.sample(frac=1)\n",
    "features = ['cement', 'blastFurnaceSlag', 'flyAsh', 'water', 'superelastizer', 'coarseAggregate', 'fineAggregate', 'age']\n",
    "target = ['compressiveStrength']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,features], df.loc[:,target], test_size=0.50, random_state=85)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_stand = pd.DataFrame(scaler.transform(X_train), columns=features)\n",
    "X_test_stand = pd.DataFrame(scaler.transform(X_test), columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_poly = RandomizedSearchCV(SVR(kernel='poly'), param_distributions={\"C\": np.logspace(-3,3,10), \"degree\": [2,3,4,5,6,7,8,9,10], \"epsilon\": np.logspace(-2, 2, 10)}, n_iter=300)\n",
    "svr_rbf = RandomizedSearchCV(SVR(kernel='rbf'), param_distributions={\"C\": np.logspace(-3,3,10), \"gamma\": np.logspace(-2, 2, 10), \"epsilon\": np.logspace(-2, 2, 10)}, n_iter=100)\n",
    "svr_poly.fit(X_train_stand[features],y_train.values.ravel())\n",
    "svr_rbf.fit(X_train_stand[features],y_train.values.ravel())\n",
    "y_pred_poly = svr_poly.predict(X_test_stand[features])\n",
    "y_pred_rbf = svr_rbf.predict(X_test_stand[features])\n",
    "mse_poly = mean_squared_error(y_test,y_pred_poly)\n",
    "mse_rbf = mean_squared_error(y_test,y_pred_rbf)\n",
    "\n",
    "print('Mean squared error SVR polynomial kernel: ' + str(mse_poly))\n",
    "print('MSE Kernel Ridge Regression using polynomial kernel: 31.36539126')\n",
    "print('Mean squared error SVR rbf kernel: ' + str(mse_rbf))\n",
    "print('MSE Kernel Ridge Regression using rbf kernel: 51.1696193149')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
