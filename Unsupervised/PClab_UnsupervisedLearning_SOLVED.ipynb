{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-lab: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Unsupervised learning_ is a different branch of machine learning, as in this case a response variable $y$ is missing. Therefore, unsupervised learning techniques are most often used for exploratory purposes or as a preprocessing step in a supervised context. Unsupervised learning is more prone to subjectivity because results are harder (or even impossible) to validate. This is why one should be careful with the interpretation of results after unsupervised learning. (Those interested can have a look at the paper [\"Clustering: Science or Art\"](http://proceedings.mlr.press/v27/luxburg12a/luxburg12a.pdf), which summarizes a couple of critics and tries to give some pointers considering the evaluation of clustering algorithms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PC-lab we will have a look at two frequently applied techniques in the context of unsupervised learning, namely principal component analysis and k-means clustering. We will end with a general scheme, in which both techniques are used. Datasets that will be used in this PC-lab are the `iris`-dataset and `digits`-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unsupervised](https://analystprep.com/study-notes/wp-content/uploads/2021/03/Img_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gaussianscatterpca](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/800px-GaussianScatterPCA.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular area of unsupervised learning is the area of _Dimensionality Reduction_, in which one tries to reduce the number of variables for visualization purposes or as a preprocessing step for clustering or classification/regression techniques. An established technique which you will find back in most statistics courses is _Principal Components Analysis_ (PCA).\n",
    "\n",
    "Assume a _column centered_ data matrix $\\mathbf{X} = [\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}]^{T}$.\n",
    "\n",
    "#### **Goal:** find the direction in $\\mathbf{X}$ with the largest variance (i.e., the most information). \n",
    "\n",
    "In other words, we need to find a linear combination of the inputs $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}$, where $\\mathbf{w}$ is also called the loadings in PCA nomenclature, for which the variance is maximized:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{arg max}_{\\mathbf{w}} \\text{Var}\\{y_{1},\\ldots,y_{N}\\}. \n",
    "\\end{equation}\n",
    "\n",
    "One can show that the above can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{arg max}_{\\mathbf{w}} \\mathbf{w}^{T}\\Sigma_{\\mathbf{X}}\\mathbf{w}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "with $\\Sigma_{\\mathbf{X}}=\\frac{1}{N-1}\\mathbf{X}^{T}\\mathbf{X}$ the sample covariance matrix. In order to avoid $\\|\\mathbf{w}\\|\\to\\infty$, we add a constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{arg max}_{\\mathbf{w}} \\mathbf{w}^{T}\\Sigma_{\\mathbf{X}}\\mathbf{w}\\,,\\quad \\text{s.t.}\\quad \\mathbf{w}^{T}\\mathbf{w}=1\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Subsequently, by using a _Lagrange multiplier_, we can obtain an _unconstrained_ convex optimization problem of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{arg max}_{\\mathbf{w}} \\mathbf{w}^{T}\\Sigma_{\\mathbf{X}}\\mathbf{w}-\\lambda(\\mathbf{w}^{T}\\mathbf{w}-1)\\,.\n",
    "\\end{equation}\n",
    "\n",
    "The above optimization problem is solved as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial}{\\partial\\mathbf{w}}\\big(\\mathbf{w}^{T}\\Sigma_{\\mathbf{X}}\\mathbf{w}-\\lambda(\\mathbf{w}^{T}\\mathbf{w}-1)\\big)=\\mathbf{0}\\,,\\\\\n",
    "\\Leftrightarrow\\quad & 2\\Sigma_{\\mathbf{X}}\\mathbf{w}-2\\lambda\\mathbf{w} = \\mathbf{0}\\,,\\\\\n",
    "\\Leftrightarrow\\quad & \\Sigma_{\\mathbf{X}}\\mathbf{w} = \\lambda\\mathbf{w}\\,,\\\\\n",
    "\\end{align*}\n",
    "\n",
    "which gives rise to the eigenvalue problem for which the solutions are given by the $r$ eigenvectors and corresponding eigenvalues of the sample covariance matrix $\\Sigma_{\\mathbf{X}}$.\n",
    "\n",
    "(Those interested in a non-formal explanation of PCA, check out this intuitive ['dining table-tale'](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) about PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 1 (warm-up): \n",
    "<br>\n",
    "a) Have a look at the <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">Iris</a> dataset. Reduce the dataset using PCA and visualize its first two components using a scatterplot. Don't forget to preprocess your data. Do you see distinctive groups? \n",
    "<br> \n",
    "b) How much variance is captured in the first three components? \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "iris = load_iris()\n",
    "X_train = iris.data\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##1a): \n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "X_transformed = pca.transform(X_train)\n",
    "var = pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "ax.scatter(X_transformed[:,0],X_transformed[:,1], c=labels, cmap=plt.cm.Set1);\n",
    "ax.set_xlabel(r'PC1 (' + str(np.round(var[0],2)*100) + '%)', size=18)\n",
    "ax.set_ylabel(r'PC2 (' + str(np.round(var[1],2)*100) + '%)', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): \n",
    "expl_var_comp012 = var[0] + var[1] + var[2]\n",
    "print('Explained variance first three components: ' + str(expl_var_comp012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): \n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(1,var.shape[0]+1),var,color='g');\n",
    "ax.set_xlabel('Principal Component', size=18)\n",
    "ax.set_ylabel('Explained variance', size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-means clustering aims to divide $n$ observations into $k$ clusters of equal variance, minimizing the within-cluster sum-of-squares. Each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. Its objective function can be written as follows, where $S = \\{S_1, ..., S_k\\}$, the set of $k$ partitions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{arg min}_S \\sum_{i=1}^k \\sum_{\\mathbf{x} \\in S_i} ||\\mathbf{x} - \\boldsymbol{\\mu}_i||^2, \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is equivalent to minimizing the pairwise squared deviations of points in the same cluster, where $\\boldsymbol{\\mu}_i$ is the mean of points in partition $S_i$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{arg min}_S \\sum_{i=1}^k \\frac{1}{2|S_i|} \\sum_{\\mathbf{x},\\mathbf{y} \\in S_i} ||\\mathbf{x} - \\mathbf{y}||^2. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-means uses the following three steps, for which step two and three are repeated until convergence is reached: \n",
    "\n",
    "1) The first step chooses the initial centroids; most easy way of doing this is by choosing $k$ samples at random from the dataset. \n",
    "\n",
    "2) In the second step each element of the dataset is assigned to its nearest centroid. \n",
    "\n",
    "3) New centroids are chosen by taking the mean of all clustered samples according to the previous centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of variations exist of the $k$-means algorithm, see for example the [$k$-medoids](https://en.wikipedia.org/wiki/K-medoids) algorithm, which uses datapoints to initialize the algorithm and works with a generalization of the Manhattan Norm to define distance between datapoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering results can be assessed, but need a different kind of metric in order to assess the performance, as typically we don't have labels at our disposal to evaluate our method. One of such metrics is the _Silhouette_-coefficient, which calculates the mean distance for a specific sample $\\mathbf{x}_i$ to all the samples in the same cluster ($a$), as opposed to the average distance to all the samples in the nearest cluster ($b$). This gives rise to a silhoutte-coefficient $\\text{si}_{\\mathbf{x}_i}$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{si}_{\\mathbf{x}_i} = \\frac{b-a}{\\text{max}\\{a,b\\}}. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this the Silhouette-index for a specific clustering of dataset $X$ can be calculated by just taking the average of all individual silhouette-coefficients: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{SI} = \\frac{1}{n} \\sum_{i=1}^n \\text{si}_{\\mathbf{x}_i}. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 2: \n",
    "<br>\n",
    "Cluster the `iris`-dataset using $k$-means. Determine $k$ using the silhouette-index; let $k$ vary from 2 to 8. Do this for the raw data and on the first two components of the PCA-transformed data. Which $k$ do you find? Visualize one of your clustering results. \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2: \n",
    "possible_k = np.arange(2,9)\n",
    "si = np.zeros(len(possible_k))\n",
    "i=0\n",
    "for k in possible_k: \n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X_train)\n",
    "    si[i] = silhouette_score(X_train, kmeans.labels_)\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2: \n",
    "si_pca = np.zeros(len(possible_k))\n",
    "i=0\n",
    "for k in possible_k: \n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X_transformed[:,0:2])\n",
    "    si_pca[i] = silhouette_score(X_transformed[:,0:2], kmeans.labels_)\n",
    "    i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(possible_k, si, label='Raw data');\n",
    "ax.scatter(possible_k, si_pca, label='PCA transformed');\n",
    "ax.set_xlabel(r'$k$', size=18)\n",
    "ax.set_ylabel('SI', size=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X_transformed[:,0:2])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_transformed[:,0],X_transformed[:,1], c=kmeans.labels_, cmap=plt.cm.Set1);\n",
    "ax.set_xlabel(r'$x_0$ (' + str(np.round(var[0],2)*100) + '%)', size=18)\n",
    "ax.set_ylabel(r'$x_1$ (' + str(np.round(var[1],2)*100) + '%)', size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining unsupervised techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will find that a number of unsupervised techniques are combined when exploratory analyses are conducted. This is typically the case when your number of variables is high, where you might suffer from the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). In these cases, the approaches laid out above can be combined using the following scheme, which can be tweaked in function of your research question:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Compute the principal components using PCA; \n",
    "\n",
    "2) Select a reduced number of components in function of the explained variance; \n",
    "\n",
    "3) Search for a number of $k$ meaningful clusters; \n",
    "\n",
    "4) Cluster your data using these final settings; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this approach and analyze a more challenging dataset, called the [`digits`-dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html). This dataset consists of handwritten images of the numbers 0-9, which has been proprocessed into feature vectors of length 64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 3: \n",
    "<br> \n",
    "a) Apply the approach illustrated above to the `digits`-dataset. Store and compare the components which explain 50% and 90% of the variance. What's the 'optimal' number of clusters? What do you think of the result? \n",
    "<br>\n",
    "b) (If you have time) Can you find a relation between the number of components you use for clustering (and thus the explained variance) and the 'optimal' number of clusters? How do you evaluate this result? \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_noc_pca(var, threshold):\n",
    "    sumvar = 0.\n",
    "    i = 0\n",
    "    while sumvar < threshold: \n",
    "        sumvar+=var[i]\n",
    "        i+=1\n",
    "    return i    \n",
    "\n",
    "\"\"\"Faster approach\n",
    "\"\"\"\n",
    "def return_noc_pca_fast(var, threshold):\n",
    "    return np.where(np.cumsum(var)>threshold)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3a): \n",
    "pca.fit(X)\n",
    "X_transformed = pca.transform(X)\n",
    "var = pca.explained_variance_ratio_\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3a):\n",
    "noc_50 = return_noc_pca(var,0.5)\n",
    "noc_90 =return_noc_pca(var,0.9)\n",
    "possible_k = np.arange(2,41,2)\n",
    "si_pca_50 = np.zeros(len(possible_k))\n",
    "si_pca_90 = np.zeros(len(possible_k))\n",
    "j=0\n",
    "\n",
    "for k in possible_k: \n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X_transformed[:,0:noc_50])\n",
    "    si_pca_50[j] = silhouette_score(X_transformed[:,0:noc_50], kmeans.labels_)\n",
    "    kmeans.fit(X_transformed[:,0:noc_90])\n",
    "    si_pca_90[j] = silhouette_score(X_transformed[:,0:noc_90], kmeans.labels_)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(possible_k, si_pca_50, label='50% principal components');\n",
    "ax.scatter(possible_k, si_pca_90, label='90% principal components');\n",
    "ax.set_xlabel(r'$k$', size=18)\n",
    "ax.set_ylabel('SI', size=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X_transformed[:,0:noc_50])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_transformed[:,0],X_transformed[:,1], c=kmeans.labels_, cmap=plt.cm.Set1);\n",
    "ax.set_xlabel(r'$x_0$ (' + str(np.round(var[0],3)*100) + '%)', size=18)\n",
    "ax.set_ylabel(r'$x_1$ (' + str(np.round(var[1],3)*100) + '%)', size=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X_transformed[:,0:noc_90])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_transformed[:,0],X_transformed[:,1], c=kmeans.labels_, cmap=plt.cm.Set1);\n",
    "ax.set_xlabel(r'$x_0$ (' + str(np.round(var[0],3)*100) + '%)', size=18)\n",
    "ax.set_ylabel(r'$x_1$ (' + str(np.round(var[1],3)*100) + '%)', size=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3b):\n",
    "\n",
    "var_dummy = 0\n",
    "poss_var = [0.2,0.4,0.5,0.6,0.7,0.75,0.8,0.85,0.9,0.95,0.98,0.99]\n",
    "k_var = np.zeros(len(poss_var))\n",
    "max_var = np.zeros(len(poss_var))\n",
    "expl_var = np.zeros(len(poss_var))\n",
    "possible_k = np.arange(2,41,2)\n",
    "\n",
    "i=0\n",
    "for v in poss_var: \n",
    "    noc = return_noc_pca(var, v)\n",
    "    j=0\n",
    "    si_pca = np.zeros(len(possible_k))    \n",
    "    for k in possible_k: \n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X_transformed[:,0:noc])\n",
    "        si_pca[j] = silhouette_score(X_transformed[:,0:noc], kmeans.labels_)\n",
    "        j+=1\n",
    "    max_var[i] = si_pca.max()\n",
    "    k_var[i] = possible_k[si_pca.argmax()]\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(poss_var, k_var);\n",
    "ax.set_xlabel('Explained variance', size=18)\n",
    "ax.set_ylabel(r'$K$', size=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
